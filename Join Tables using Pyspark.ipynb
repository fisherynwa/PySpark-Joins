{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc44b77d-83a9-4ebf-93f3-1ecef53d1adb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ETL Pipeline: Joining Geographic Data (Countries, Regions, Sub-Regions)\n",
    "\n",
    "\"\"\"\n",
    "             ┌──────────────────────────────┐\n",
    "             │         df_regions            │\n",
    "             │  (load and select)            │\n",
    "             │  id        |   region         │\n",
    "             └────────────┴─────────────────┘\n",
    "                       ▲\n",
    "                       │ (df_countries.region_id = df_regions.id)\n",
    "                       │\n",
    "┌─────────────────────────────┐\n",
    "│        df_countries          │\n",
    "│   (load and select)          │\n",
    "│  country_id   |   country    │\n",
    "│  region_id    |   sub_region_id\n",
    "│  population   |   area_km2   │\n",
    "└───────────────┴─────────────┘\n",
    "                       │\n",
    "                       │ (df_countries.sub_region_id = df_sub_regions.id)\n",
    "                       ▼\n",
    "             ┌─────────────────────────────┐\n",
    "             │       df_sub_regions         │\n",
    "             │  id        |   sub_region    │\n",
    "             └────────────┴────────────────┘\n",
    "\"\"\"\n",
    "\n",
    "print(\"The following cells execute the scheme given above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68107bb1-39b6-462b-b255-6cb7a5d2b855",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The following steps are applied:\n",
    "\n",
    "- load the country data\n",
    "- adopt a clean schema (it is efficient and defines types for each column)\n",
    "- select only the useful columns\n",
    "- rename some columns for clarity\n",
    "- display the first 5 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4819028d-4ef1-4b7b-99d9-7dad92013c19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load country data\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType\n",
    ")\n",
    "\n",
    "schema_countries = StructType([\n",
    "    StructField(\"country_id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"nationality\", StringType(), True),\n",
    "    StructField(\"country_code\", StringType(), True),\n",
    "    StructField(\"iso_alpha2\", StringType(), True),\n",
    "    StructField(\"capital\", StringType(), True),\n",
    "    StructField(\"population\", IntegerType(), True),\n",
    "    StructField(\"area_km2\", IntegerType(), True),\n",
    "    StructField(\"region_id\", IntegerType(), True),\n",
    "    StructField(\"sub_region_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df_countries = (\n",
    "    spark.read\n",
    "         .csv(\n",
    "             \"/Volumes/customer_orders/default/countries/countries_population.csv\",\n",
    "             header=True,\n",
    "             schema=schema_countries\n",
    "         )\n",
    "         .select(\n",
    "             \"country_id\",\n",
    "             col(\"name\").alias(\"country\"),\n",
    "             \"region_id\",\n",
    "             \"sub_region_id\",\n",
    "             \"population\",\n",
    "             \"area_km2\"\n",
    "         )\n",
    ")\n",
    "\n",
    "df_countries.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e723de3-cb1c-46f2-84c1-433e0b31254f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load regions data\n",
    "\n",
    "schema_regions = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_regions = (\n",
    "    spark.read\n",
    "         .csv(\n",
    "             \"/Volumes/customer_orders/default/countries/country_regions.csv\",\n",
    "             header=True,\n",
    "             schema=schema_regions\n",
    "         )\n",
    "         .select(\n",
    "             \"id\",\n",
    "             col(\"name\").alias(\"region\")\n",
    "         )\n",
    ")\n",
    "\n",
    "df_regions.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77bb0985-c2cd-4a4a-84f7-263c83aa7405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load sub-regions data\n",
    "\n",
    "schema_sub_regions = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_sub_regions = (\n",
    "    spark.read\n",
    "         .csv(\n",
    "             \"/Volumes/customer_orders/default/countries/country_sub_regions.csv\",\n",
    "             header=True,\n",
    "             schema=schema_sub_regions\n",
    "         )\n",
    "         .select(\n",
    "             \"id\",\n",
    "             col(\"name\").alias(\"sub_region\")\n",
    "         )\n",
    ")\n",
    "\n",
    "df_sub_regions.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a60a6313-9700-4216-8933-bc4da43026bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " The main cell of the notebook joins the three DataFrames by means of the region_id and sub_region_id keys.\n",
    " The (\".\") operation allows us to chain multiple PySpark operations together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea9d35c5-c94d-4037-915b-70419f79ef7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join the three DataFrames\n",
    "\n",
    "df_joined = (\n",
    "    df_countries\n",
    "        .join(\n",
    "            df_regions,\n",
    "            df_countries.region_id == df_regions.id,\n",
    "            \"left\"\n",
    "        )\n",
    "        .select(\n",
    "            \"country_id\",\n",
    "            \"country\",\n",
    "            \"region\",\n",
    "            \"population\",\n",
    "            \"area_km2\",\n",
    "            \"sub_region_id\"\n",
    "        )\n",
    "        .join(\n",
    "            df_sub_regions,\n",
    "            df_countries.sub_region_id == df_sub_regions.id,\n",
    "            \"left\"\n",
    "        )\n",
    "        .select(\n",
    "            \"country_id\",\n",
    "            \"country\",\n",
    "            \"region\",\n",
    "            \"sub_region\",\n",
    "            \"population\",\n",
    "            \"area_km2\"\n",
    "        )\n",
    ")\n",
    "\n",
    "df_joined.show(5)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Join Tables using Pyspark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
